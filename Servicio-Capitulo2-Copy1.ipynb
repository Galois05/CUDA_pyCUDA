{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capítulo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya logramos escribir nuestro primer programa en `CUDA C`, pasaremos a un conocimiento un poco más profundo sobre el funcionamiento del `hardware` al programar en paralelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalabilidad Transparente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya habíamos mencionado antes, es importante que los programas y aplicaciones escritos en `CUDA C` sean compatibles con distintos tipos de `hardware`. Esto quiere decir, sobre todo, con distintas tarjetas gráficas. \n",
    "\n",
    "`CUDA` tiene la ventaja de una gran flexibilidad a la hora de organizar y ejecutar los `blocks`. Esto quiere decir, tal y como se muestra en la figura siguiente, que estos últimos pueden ser colocados en varias columnas de pocos blocks o en un número pequeño de columnas con muchos `blocks`. Esto hace que el tiempo de ejecución varíe de una tarjeta gráfica a otra, sin embargo hay otros parámetros con los que se puede jugar.\n",
    "\n",
    "Al igual que la organización de los `blocks`, también se puede jugar con los tiempos de ejecución de estos. Es decir, siguiendo la figura mostrada aquí abajo que el bloque 5 puede ser ejecutado antes o simultaneamente que el bloque 1. Esto dependerá de distintos factores que se verán más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=Escalabilidad1.png align=cente> <img src=Escalabilidad2.png align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMD, SM y Warps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el capítulo pasado discutimos el modelo de *von Neumann* que hay en los `CPU` y `GPU`. Para estos últimos, este modelo se complementa de un `SIMD` o *Single Instruction Multiple Data*. Estos permiten que los `threads` que sean ejecutados en nuestro procesador hagan una única función. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quien se ocupa de ordenar y ejecutar los `blocks` son los `SM` o *Streaming Multiprocessors*. Ellos dan y mantienen los índices a los `threads` y a los `blocks`. Estos tienen la caracteristica de sólo permitir la entrada de 8 bloques y de 1536 `threads`, por lo que nos restringe la dimensión de los bloques con los que trabajaremos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo: ¿Qué es más eficaz: bloques de 8x8, 16x16 o 32x32?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supongamos que trabajaremos con bloques de 8x8. Esto quiere decir que cada bloque tendrá 64 `threads`. Para llenar un `SM` entonces necesitaremos 1536/64 = 24 bloques, sin embargo sólo podremos utilizar 8 debido a las restricciones que nos pone la `SM`, haciendo que descartemos 16 bloques y trabajemos únicamente con 8 bloques, es decir 512 `threads`. Esta cantidad representa únicamente 1/3 de la capacidad de los `SM`.\n",
    "\n",
    "- Supongamos ahora que tenemos bloques de 16x16. Esto implica que cada uno de estos tendrá 256 `threads`. Al dividir 1536 entre 256 obtenemos un resultado de 6, por lo que necesitaremos 6 bloques (debajo del límite permitido) para llenar el espacio de un `SM`, utilizando así toda su capacidad.\n",
    "\n",
    "- Finalmente tenemos el caso de 32x32, lo cual significa que cada bloque tendrá 1024 `threads`. Esto hace que sólo podamos utilizar un bloque en el `SM`, haciendo que 512 espacios para `threads` queden desocupados, haciendo finalmente que utilicemos 2/3 de la capacidad total del `SM`.\n",
    "\n",
    "En conclusión podemos ver como la elección de la dimensión de los bloques afecta en la capacidad y eficacia de nuestro programa. Por lo que siempre hay que detenerse a pensar sobre la dimensión más apta según el problema que tengamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota 1:** El número máximo de `threads` que puede soportar un `SM` enlistado aquí es para los `SM` *Fermi*.\n",
    "\n",
    "**Nota 2:** En versiones anteriores a `CUDA 3.0` un bloque puede tener como máximo 512 `threads`, por lo que en el caso de estas versiones los `blocks` de 32x32 no podrán ser ejecutados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *Warps* son otra división \"natural\" de los `threads` dentro de los bloques. Cada `Warp` está conformado por 32 `threads` y al igual que estos últimos y los `blocks`, los `warps` también están indexados. Por lo que el `warp` 0 empieza en el mismo lugar que el `thread` 0 y termina en el `thread` 31. \n",
    "¿Qué ocurre en el caso de bloques tridimensionales? Se hace un mapeo a una dimensión en analogía a la fórmula utilizada en los códigos anteriormente escritos ` int i = blockIdx.x * blockDim.x + threadIdx.x `.\n",
    "\n",
    "Cuando el código es compilado y ejecutado, el `kernel` busca `warps` que estén listos en un primer tiempo. Este no puede ejecutar a todos los `warps` al mismo tiempo, sino que ejecuta a una pequeña cantidad. A estos último se les llama `ready warps` o simplemente `warps listos`. Estos son ejecutados en un primer tiempo debido a que todos sus operandos ya están listos y pueden proceder a la ejecución. La no ejecución de los demás `warps` se debe a que sus operandos aún no están listos. Estos estarán listos cuando la memoria global les dé la información suficiente para ejecutarse, o cuando una `ULA` le de la información sobre una operación que esté realizando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divergencia de Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los distintos `threads` dentro de un `warp` pueden estar o no estar sincronizados. Esto significa que pueden o no estar haciendo la misma operación. Esto afecta directamente en el funcionamiento del programa en cuestión. Si por ejemplo la mitad de los `threads` dentro de un `warp` está realizando una operación A mientras que la otra mitad una operación B, la eficacia del código se ve afectada a comparación del caso en el que todos los `threads` de un `warp` realizan una sola operación. \n",
    "A esta falla en el funcionamiento se le llama *Divergencia de Control*.\n",
    "\n",
    "**Ejemplo:** \n",
    "+ Imaginemos un código en el cual encontremos un condicional de la forma: ` if (threadIdx.x > 2) {} `. Esto significa que en el primer `warp` de nuestro bloque, los primeros 3 `threads` harán una cierta operación, mientras que los otros 29 harán otra. En este caso habrá una divergencia de control.\n",
    "\n",
    "+ Esta divergencia puede evitarse con condicionales de esta forma: ` if (blockIdx.x > 2) {} `. En este caso todos los `warps` en el bloque 0, 1 y 2 harán una operación, mientras que los demás bloques harán otra (nota como el número de `threads` que un bloque puede contener es multiplo de 32 según el ejemplo anterior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresemos ahora al caso de nuestra multiplicación de vectores. Supongamos que `n = 1000`. Recordemos parte del código:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```C\n",
    "__global__ void multiplicacion_vectores( float * d_A, float * d_B, float * d_C, int n) {\n",
    "\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x ;\n",
    "    if (i<n) { \n",
    "                d_C[i] = d_A[i] * d_B[i] ; \n",
    "             }\n",
    "    } \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cubrir las 1000 entradas de los vectores, utilizaremos 32 `warps` que nos darán 1024 `threads`. Como puede intuirse ya, tendremos una divergencia de control en (y sólo en) el último `warp`. Esto debido a que los primeros 8 `threads` harán la operación en el `kernel`, mientras que el resto no. Por ahora no podemos hacer nada al respecto para eliminar esta divergencia.\n",
    "\n",
    "Es importante notar que esta divergencia no es lo suficientemente grande para que esta pueda afectar gravemente el funcionamiento de nuestro código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos de Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen distintos tipos de memoria en un `GPU`. Sin embargo, antes de nombrarlos empezaremos con una pequeña analogía para entender mejor qué ocurre en el `hardware`. \n",
    "\n",
    "Supongamos un salón de clase en el que se encuentran 24 niños distribuidos en 4 mesas con 6 sillas cada una. El profesor les deja como tarea el solucionar un problema. Sin embargo la resolución del problema tiene que ser hecha en 4 partes distintas. Cada una de estas partes es asignada a sólo una mesa, y los niños que conforman esta han de trabajar en equipo. \n",
    "De esta manera, un niño puede leer y escribir en su cuaderno, en una pequeña pizarra asignada a cada mesa y en el gran pizarrón que se encuentra en el salón. \n",
    "Sin embargo hay dos restricciones: los niños de una mesa no pueden ni leer ni escribir en la pizarra de otra mesa y ningún niño puede leer ni escribir en el cuaderno de otro niño.\n",
    "\n",
    "La memoria del `GPU` funciona de la misma manera. Si los niños fueran `threads`, la mesa un `block` y el salón una `grid`, entonces:\n",
    "\n",
    "+ El cuaderno sería el  `register`  o *registro*, en el cual sólo un `thread` puede leer y escribir sus variables locales.\n",
    "+ La pizarra asignada a cada mesa sería la  `shared memory`  o *memoria compartida*, a la que pueden acceder cada `thread` dentro de un bloque. Sin embargo los `threads` de otro bloque no.\n",
    "+ El pizarrón del salón sería la  `global memory`  o *memoria global*, en la que pueden acceder *todos* los `threads`.\n",
    "+ Además, en el `GPU` existe también la  `constant memory`  o *memoria constante*, que es muy similar a la memoria global.\n",
    "<img src=Memorias.png align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables ` __device__ `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera llegamos a los distintos tipos de variables ` __device__ `. Encontramos 4, las cuales son:\n",
    "\n",
    "+ Variable Local, la cual es declarada como ` int LocalVar ;`. Esta vive en el `registro` y puede ser escrita y leída únicamente por un `thread`. Su tiempo de vida es igual al tiempo de vida del `thread` al cual está atada, lo que significa que es declarada cuando el `thread` es ejecutado y eliminada cuando este muere.\n",
    "+ Variable Compartida, la cual es declarada como ` __device__ __shared__ int SharedVar ; `. Esta vive en la `memoria compartida`. Un mismo bloque puede leerla y modificarla, mientras que su tiempo de vida es igual al del bloque.\n",
    "+ Variable Global, la cual es declarada como ` __device__ int GlobalVar ; `. Esta vive en la `memoria global` y puede ser leída y escrita por toda la `grid`. Su tiempo de vida es igual al de la ejecución del programa.\n",
    "+ Constante, la cual es declarada como ` __device__ __constant__ int ConstantVar ; `. Tiene las mismas caracteristicas que la variable global, con la diferencia que vive en la `memoria constante`.\n",
    "\n",
    "**Nota:** en el caso de las variables compartidas y constantes, el ` __device__ ` en la declaración puede ser omitido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:** Adelantándonos un poco a  lo que se verá más adelante, presentamos parte de un `kernel` para multiplicación de matrices en el que utilizaremos la `memoria compartida`. Sin prestarle atención a los detalles, este ejemplo tiene la única intención de mostrar como se declaran `variables compartidas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```C\n",
    "\n",
    "__global__ void MatrixMulKernel( int m, int n, int k, float * A, float * B, float * C ) { \n",
    "    \n",
    "      __shared__ float ds_A[TILE_WIDTH][TILE_WIDTH] ; \n",
    "      __shared__ float ds_B[TILE_WIDTH][TILE_WIDTH] ; \n",
    "    \n",
    "    .\n",
    "    .\n",
    "    .\n",
    "\n",
    "    } \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una pregunta valida en este punto sería ¿por qué vemos hasta este momento este tipo de variables?, ¿que no ya habíamos declarado variables similares en la multiplicación de vectores?\n",
    "\n",
    "La diferencia entre las variables que utilizamos en el código de adición de vectores fueron dinámicas, mientras que en este caso estamos utilizando variables estáticas. Por lo que anteriormente podíamos prescindir de este tipo de variables, gracias a las funciones `cudaMalloc` y `cudaMemcpy`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Es **importante** hacer notar que los cambios que hace un `thread` en las variables compartidas o globales no son vistos *inmediatamente* por los demás, por lo que es importante asegurarse de que un `kernel` haya terminado antes de iniciar otro. Esto para estar seguro de que los cambios realizados por uno puedan ser vistos por el siguiente. El cómo evitar esto será visto prontamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Dónde declarar las variables?**\n",
    "\n",
    "El procedimiento es muy sencillo. Uno debe preguntarse: **¿El `host` podrá acceder a estas?**\n",
    "\n",
    "+ Sí: entonces la variable es global o constante, por lo que debe de ser declarada afuera de cualquier función.\n",
    "+ No: entonces la variable es local o un registro, por lo que debe de ser declarada dentro del `kernel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cuales son las ventajas de tener tantos tipos de memoria?** \n",
    "\n",
    "La respuesta será descubierta conforme al avance de nuestros conocimientos en `CUDA C`, sin embargo como un pequeño adelanto podemos mencionar que la velocidad en que un `thread` accede a la información de la memoria compartida es de 1 a 2 órdenes de magnitud mayor que la velocidad en la que accede a la memoria global. Por lo que a la hora de escribir un código de un nivel avanzado, estos factores cuentan en la eficiencia y el funcionamiento de nuestro programa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicación de Matrices (ejemplo básico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cualquiera que haya ya hecho un código de multiplicación de matrices recordará que la idea es muy básica. Lo mismo será en el caso de CUDA C.\n",
    "\n",
    "Sin embargo vale la pena poner especial atención a la manera de indexar los `threads` y como se forman las columnas y renglones de la matriz.\n",
    "\n",
    "También es importante notar que en este código ya pasaremos a dos dimensiones, `x` y `y`. No hay de qué preocuparse.\n",
    "\n",
    "En primero lugar, y únicamente para hacernos la vida un poco más sencilla, escribiremos:\n",
    "```C\n",
    "\n",
    "tx = threadIdx.x ; ty = threadIdx.y ;\n",
    "bx = blockIdx.x ; by = blockIdx.y ;\n",
    "```\n",
    "\n",
    "Ahora los indices `Fila` y `Col`. Básicamente estos son iguales al índice `idx` con el que jugamos en el `kernel` de multiplicación de vectores, con la diferencia que ahora tendremos dos por las dos dimensiones del problema.\n",
    "\n",
    "De esta manera: \n",
    "\n",
    "```C\n",
    "\n",
    "Fila = by * blockDim.y + ty ;\n",
    "Col = bx * blockDim.x + tx ;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hablemos del algoritmo. En este ejemplo las matrices no estarán hechas como un arreglo de dos dimensiones, sino se mapeará hacia un arreglo de una dimensión (revisar la plantilla escrita para este ejercicio un poco más abajo). Esto quiere decir que tendremos que pasar de la indexación `[Fila][Columna]` a una en una dimensión.\n",
    "\n",
    "El razonamiento es exactamente el mismo con el que se escribió el índice `idx`, `Fila` y `Col`. Eligiremos a una fila y la limitaremos por el numero de columnas que hay. A eso le sumaremos el índice dinámica que se mueve por las columnas en esa fila.\n",
    "Nuestra nueva indexación queda como `[Fila * numCol + i]`, donde numCol es el número de columnas que la matriz tiene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo queda entonces así:\n",
    "\n",
    "```C\n",
    "if (Fila<numFilasC && Col<numColC) {\n",
    "    float Cvalue = 0.0 ;\n",
    "    for (int i = 0 ; i < numColA ; i ++) {\n",
    "        Cvalue += A[Fila * numColA + i] * B[Col + i*numColB] ;\n",
    "        __syncthreads() ;\n",
    "        }\n",
    "    C[Fila * numColC + Col] = Cvalue ;\n",
    "    __syncthreads() ;\n",
    "    }\n",
    "```\n",
    "\n",
    "Nótese los cambios que hay en el índice de la matriz B. También es importante notar la misteriosa función `__syncthreads() ` que incluímos al final de los cálculos. Esto será explicado a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Cada `thread` funciona de manera independiente, por lo que no podemos asegurar que todos tomen el mismo tiempo para hacer las operaciones que deben. Esto puede provocar en algunos códigos que funciones dentro del kernel empiecen cuando la información que van a procesar no está completa. \n",
    " \n",
    " Siguiendo el ejemplo de los niños dentro del salón. Supongamos que llega la hora de la comida, y cada uno de los niños debe tomar el camión escolar para regresar a su casa. \n",
    " Imaginemos por un momento que el camión deje a uno de los niños en la escuela. Esto le provocaría problemas tanto al chofer del camión como a la escuela por la queja de los padres. Para evitar esto, el chofer debe de asegurarse que **todos** los niños estén dentro del camión antes de que pueda arrancar. \n",
    " Regresando al caso de la computación en paralelo, los niño siguen siendo los `threads`, y el chofer que se asegura que todos los niños estén en el camión es la función ` __syncthreads() `. El no poner esta función equivaldría a que el chofer pudiera olvidar a uno de los niños. \n",
    " \n",
    " De esta manera, al colocar la función ` __syncthreads() ` dentro del kernel nos aseguramos que todos los threads que están calculando un coeficiente $C_{y,x}$ terminen de calcularlo antes de pasar al cálculo de otro coeficiente $C_{y',x'}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El kernel no tiene, a parte de las funciones ` __syncthreads() `, gran diferencia con respecto a un código escrito en cualquier lenguaje de programación serial. Los condicionales `if` son para asegurarnos que no se sobreescriba información en el cálculo de $C$. \n",
    "\n",
    "<img src=Syncthreads.png align=center>\n",
    "<h4 align=center >Ilustración de como funciona ` __syncthreads()`. Esta está representada aquí como la envoltura que no deja seguir a los `threads` a la siguiente etapa hasta que el último (`thread N-2`) termine sus operaciones<h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin más, pasemos a escribir el código completo de una multiplicación de matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__global__ void multiplicacion_matrices()\n",
    "{\n",
    "    // llenar el kernel\n",
    "    \n",
    "}\n",
    "\n",
    "int main( int argc, char * argv[])\n",
    "{\n",
    "    \n",
    "    int numColA 1000 ;\n",
    "    int numColB 1000 ;\n",
    "    int numColC 1000 ;\n",
    "    \n",
    "    int numFilaA 1000 ;\n",
    "    int numFilaB 1000 ;\n",
    "    int numFilaC 1000 ;\n",
    "    \n",
    "    float A[numFilaA*numColA] ;\n",
    "    float B[numFilaB*numColB] ;\n",
    "    float C[numFilaC*numColC] ;\n",
    "    \n",
    "    for (i = 0 ; i < numFilaA*numColA ; i ++)\n",
    "    {\n",
    "        A[i] = i ;\n",
    "    }\n",
    "\n",
    "    for (i = 0 ; i < numFilaB*numColB ; i ++)\n",
    "    {\n",
    "        B[i] = i + 1 ;\n",
    "    }\n",
    "    \n",
    "    // Escribe abajo las lineas para la alocacion de memoria\n",
    "    \n",
    "    // Escribe abajo las lineas para la copia de memoria del CPU al GPU\n",
    "    \n",
    "    // Escribe las dimensiones de los bloques y mallas\n",
    "    \n",
    "    // Escribe las lineas para lanzar el kernel\n",
    "    \n",
    "    // Copia la memoria del GPU al CPU\n",
    "      \n",
    "    // Libera la memoria\n",
    "      \n",
    "    // Escribe un codigo para poder saber si tu resultado es correcto\n",
    "    \n",
    "    \n",
    "    return 0 ;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicación de Matrices con un `kernel` en bloques: una Introducción a programación en bloques (Tiled Programmation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección nos dedicaremos a introducir un método de programación mucho más eficiente. En inglés es llamada *tiled programmation* y no hay una traducción exacta en español. Por eso la llamaremos *programación en bloques*, aunque he de admitir que es un pésimo nombre debido a que se presta a confundir los bloques de información, `tiles`, con los bloques de hilos, `thread blocks` o simplemente `blocks`.\n",
    "Por lo que de ahora en adelante habrá de recodar que cuando se habla de programación en bloques, estos últimos se refieren a bloques de **información**.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo concreto del anterior `kernel` de multiplicación de matrices, cada dato de las dos matrices se encuentra en la `memoria global` y es utilizado por el `kernel` en distintas ocasiones. La idea de la programación en bloques es reducir y ordenar el número de accesos a los datos con ayuda de los distintos tipos de memoria que se encuentran en el GPU. Esto se observa mejor en las figuras siguientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=Global.png align=center> \n",
    "<h4 align=center >La ilustración superior muestra el proceso cuando no programamos en bloque. En las imágenes inferiores pasamos a un proceso por `tiles`.</h4> \n",
    "<img src=Tiled.png align=center>\n",
    "<h4 align=center >Fase 1 de la programación en bloques. En este caso la memoria `on-chip` equivale a la `shared memory`</h4>\n",
    "<img src=Tiled2.png align=center>\n",
    "<h4 align=center >Fase 2 de la programación en bloques.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos entonces que los `threads` 1 y 2 utilizan 12 datos guardados en la memoria **global**. Para optimizar los cálculos, estos los haremos en 3 fases.\n",
    "En la primera guardamos sólo 4 datos en un bloque en la memoria **compartida**. Ahora los threads 1 y 2 tendrán un acceso más rápido a los datos que si estos estuvieran en la memoria global. Sin embargo hay que recordar que la memoria compartida tiene un tiempo de vida igual al `thread block`, por lo que cuando los `threads` 1 y 2 terminen con los 4 datos, estos serán eliminados de la memoria compartida. \n",
    "Ahora entramos en la fase 2, en la que guardamos los siguientes 4 datos en la memoria compartida. repitiéndose lo mismo que en la fase 1.\n",
    "Así hasta la fase 3. \n",
    "\n",
    "**Siguiendo la analogía...**\n",
    "\n",
    "Pensemos ahora que cada uno de los niños ya están en su casa después de un viaje en el camión escolar y están trabajando duramente en sus tareas, sin embargo ahora son sus padres quienes se preocupan en lo que prepararán para cenar, por lo que tienen que ir al super mercado para comprar los ingredientes con los que se preparará la comida.\n",
    "En muchas ciudades del mundo, la distribución de algunos productos se encuentra centralizado en algunos barrios de la ciudad, por lo que encontrar algunos productos requiere de viajar grandes distancias hasta el lugar en el que se encuentra el producto deseado.\n",
    "Sin embargo en algunas otras ciudades se han implementado políticas para descentralizar estos mercados, por lo que si una familia vive en algún barrio suburbano, esta no tenga que viajar hasta el centro de la ciudad para conseguir algunos productos. Esto le permite a las personas optimizar sus viajes.\n",
    "\n",
    "La idea de la programación en bloques es la misma. Los `threads` necesitan operandos (comida) para trabajar. Al programar en bloque uno lleva la comida al super mercado local en el que se encuentra el `thread` de tal manera que este no tenga que viajar al centro de la ciudad. \n",
    "\n",
    "Sin embargo podemos encontrar dos distintos tipos de problemas que pueden presentarse en estos casos. \n",
    "Uno es cuando en el barrio local, cada una de las familias que viven ahí no buscan los mismos productos, es decir que en un `bloque de hilos`, cada uno de estos no busca los mismos operandos. \n",
    "Otro es cuando el supermercado local no tiene la suficiente infraestructura para darle cábida a toda la comida necesaria, i.e. no hay el suficiente espacio en la `memoria compartida` para traer todos los operandos necesarios.\n",
    "\n",
    "En el primer caso, hay que encontrar una serie de datos la cual sea utilizada por múltiples `threads`, asegurándonos que cada dato será utilizado. Para luego pasar a la siguiente serie de datos, una vez que los `threads` hayan hecho sus operaciones.\n",
    "\n",
    "El segundo caso tiene que ver con la capacidad de la memoria compartida. Podemos darle a esta dos configuraciones:\n",
    "16 KB de memoria compartida 48KB en el `cache L1`. Esto nos permite dar de nuevo los detalles a la hora de elegir el tamaño del `bloque`, o viceversa. Tomemos aquella con 16KB de memoria compartida.\n",
    "\n",
    "+ Supongamos que elegimos de nuevo un tamaño de `thread block` de 16x16. Puesto que cada `thread` carga dos datos (uno de la matriz $A$ y otro de la matriz $B$) y cada uno de estos pesa 4B, entonces obtenemos que cada uno de los bloques utiliza 256x2x4B = 2KB, por lo que podremos utilizar los 8 bloques permitidos para llenar los espacios permitidos en el `SM`.\n",
    "+ Si ahora utilizamos un `bloque` de 32x32, entonces cada bloque usará 1024x2x4B = 8KB, por lo que únicamente podremos utilizar 2 bloques. Sin embargo, cada uno de los bloques tiene 1023 `threads`, resultando que sólo podremos utilizar uno debido a la restricción de nuesto `SM` de almacenar 1536 `threads` como máximo.\n",
    "\n",
    "De esta manera la dimensión del bloque y el número de operaciones hechas por los `threads` son a considerarse a la hora de utilizar la programación en bloques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Después de todas estas restricciones y detalles en los que hay que pensar, uno llega a preguntarse: ¿realmente vale la pena este tipo de programación?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto puede contestarse en parte por los siguientes datos:\n",
    "\n",
    "En general un `GPU` tiene un máximo de 1000GFLOPS.\n",
    "En nuestro kernel básico de multiplicación de matrices, cada `thread` utiliza 4B por cada dato, por lo que si buscamos tener 1 FLOPS, entonces necesitamos una velocidad de 4B/s, por lo que si buscamos maximizar la velocidad de nuestro código, entonces estamos esperando un ancho de banda de 1000 operaciones x 4B/s = 4000GB/s.\n",
    "Ahora bien, con un `GPU` estándar como en el que se corrió este código tenemos un ancho de banda entre los threads y la memoria global de 150GB/s, dándonos un total de 37.5GFLOPS cuando dividimos esta entre 4. Sin embargo a la hora de correr el código obtenemos un total de aproximadamente 25GFLOPS. \n",
    "\n",
    "En el caso de nuestro kernel con programación en bloque, el hecho de poner como intermediario a la `memoria compartida` provoca que el acceso a la `memoria global` descienda por un factor de 16 en el caso de los `blocks` de 16x16 y de 32 en el caso de 32x32. Haciendo de nuevo los cálculos para el caso de 16x16 (el cual ya vimos que es el más óptimo) tenemos entonces que el número máximo de GFLOPS al que podremos llegar es (150/4)16 = 600 GFLOPS!!!! Este número es mucho más cercano al máximo de nuestro `GPU`. \n",
    "\n",
    "Esto es únicamente una pequeña discusión sobre los beneficios de nuestro código en bloques, sin embargo es importante ver e interactuar con los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin más rodeos, presentamos el `kernel` de la multiplicación de matrices con bloques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```C\n",
    "#define TILE_WIDTH 16\n",
    "\n",
    "__global__ void matrixMultiplyShared(float *A, float *B, float *C, int numFilasA,\n",
    "                                     int numColA, int numColB) { \n",
    "\n",
    "\t__shared__ float ds_A[TILE_WIDTH][TILE_WIDTH] ;\n",
    "\t__shared__ float ds_B[TILE_WIDTH][TILE_WIDTH] ;\n",
    "\n",
    "\tint bx = blockIdx.x ; int by = blockIdx.y ;\t\n",
    "\tint tx = threadIdx.x ; int ty = threadIdx.y ;\n",
    "\n",
    "\tint Fila = by * blockDim.y + ty ;\n",
    "\tint Col = bx * blockDim.x + tx ;\n",
    "\tfloat Cvalue = 0 ;\n",
    "\n",
    "\tfor (int t=0; t<(numColA-1)/TILE_WIDTH+1; t++) {\n",
    "\n",
    "\t\tif (Fila<numFilasA && t*TILE_WIDTH+tx<numColA) {\n",
    "\t\t\tds_A[ty][tx] = A[Row*numColA + t*TILE_WIDTH +tx] ;\n",
    "\t\t} else {\n",
    "\t\t\tds_A[ty][tx] = 0.0 ;\n",
    "\t\t}\n",
    "\n",
    "\t\tif (t*TILE_WIDTH+ty<numColA && Col<numColB) {\n",
    "\t\t\tds_B[ty][tx] = B[(t*TILE_WIDTH +ty)*numColB + Col] ;\n",
    "\t\t} else {\n",
    "\t\t\tds_B[ty][tx] = 0.0 ;\n",
    "\t\t}\n",
    "\t\t__syncthreads() ;\n",
    "\t\t\n",
    "\t\tfor (int i=0; i < TILE_WIDTH; i ++) {\n",
    "\t\tCvalue += ds_A[ty][i] * ds_B [i][tx] ;\n",
    "\t\t}\n",
    "\t\n",
    "\t__syncthreads() ;\n",
    "\tif (Fila<numFilasA && Col<numColB) {C[Fila*numColB + Col] = Cvalue ;}\n",
    "\t}\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este código hay más detalles a comentar que en el caso pasado. \n",
    "\n",
    "+ El parámetro `t` utilizado en el `for` nos da el número de `tile` en el cual estamos trabajando. El número máximo de bloques o `tiles` con los que trabajaremos está dado, en el caso de una multiplicación de matrices de mxn y nxk por `(n-1)/TILE_WIDTH + 1`; algo así como el algoritmo que utilizamos en el capítulo anterior para saber la dimensión de nuestra `grid`.\n",
    "\n",
    "+ Los dos primeros condicionales `if/else` son condiciones de frontera y nos permiten almacenar la cantidad exacta de datos. Supongamos que nuestra matriz $A$ es de 256x257, entonces necesitaremos de 17 `tiles` en la dirección x, haciendo que 12 `threads` no tengan asignados ningún valor de la matriz $A$. Para evitar que tengan información no deseada, hacemos a todos estos sobrantes 0.0. Pasa lo mismo para la matriz $B$.\n",
    "\n",
    "+ la variable `Cvalue` está definida afuera de los condicionales y los loops. Esto podemos hacerlo gracias al uso de `tiles`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: escribir los códigos de multiplicación de matrices utilizando los dos kernels presentados anteriormente y comparar la rapidez de los resultados.\n",
    "\n",
    "\n",
    "Probado localmente, se utilizaron dos matrices con dimensiones 200x100 y 100x256. Al correr nuestro código con el `kernel` básico, el cálculo se realizó en 0.24 ms.\n",
    "Cuando se compiló el código con un `kernel` en bloques o `tiled kernel`, el computo se realizó en  ¡¡¡0.13ms!!!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.5",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
