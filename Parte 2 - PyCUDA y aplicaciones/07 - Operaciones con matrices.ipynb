{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplicación de matrices en PyCUDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la primer parte de las notas, dedicada a CUDA tuvimos la oportunidad de ver códigos para llevar a cabo la multiplicación de matrices. Ahora es tiempo de hacer lo propio, pero en PyCUDA. Mostramos dos códigos, uno con el algoritmo usual y el segundo con el algoritmo de multiplicación por teselas. A continuación mostramos los dos códigos con sus respectivos tiempos de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Matriz A (GPU):\n",
      "[[ 0.42087126 -0.45696247  1.40747416 -0.01336118]\n",
      " [-0.27291611  0.54827404 -0.95525765 -0.16676982]\n",
      " [ 1.22548676 -0.35120484 -0.13768797 -1.89706707]\n",
      " [-1.2680856  -0.30036467 -0.58157474  0.50693828]]\n",
      "--------------------------------------------------------------------------------\n",
      "Matriz B (GPU):\n",
      "[[-0.71905339  1.20476782 -0.23252736  0.3357895 ]\n",
      " [-1.27571738  1.37000728  0.8551234  -0.94778675]\n",
      " [ 0.36710623  0.4699367   0.88681698 -1.34979498]\n",
      " [ 0.14133705 -1.00428557 -1.04021072 -0.0901983 ]]\n",
      "--------------------------------------------------------------------------------\n",
      "Matriz C (GPU):\n",
      "[[ 0.79513013  0.55585241  0.77344704 -1.32416928]\n",
      " [-0.87745327  0.14091277 -0.14136051  0.69315511]\n",
      " [-0.75182426  2.83576632  1.26596272  1.10133564]\n",
      " [ 1.15315115 -2.72166443 -1.00505722  0.59815353]]\n",
      "--------------------------------------------------------------------------------\n",
      "Diferencia CPU-GPU:\n",
      "[[ -5.96046448e-08   0.00000000e+00  -5.96046448e-08   0.00000000e+00]\n",
      " [ -5.96046448e-08  -2.98023224e-08   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00  -1.19209290e-07]\n",
      " [  1.19209290e-07   0.00000000e+00  -1.19209290e-07  -5.96046448e-08]]\n",
      "CPU times: user 70.2 ms, sys: 59.5 ms, total: 130 ms\n",
      "Wall time: 79.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "\n",
    "# -- inicializamos el dispositivo\n",
    "import pycuda.autoinit\n",
    "\n",
    "plantilla_codigo_kernel = \"\"\"\n",
    "__global__ void MatrizMulKernel(float *a, float *b, float *c)\n",
    "{\n",
    "    // 2D Thread ID (suponiendo que solo se ejecuta *un* bloque)\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "\n",
    "    // Pvalor se usa para guardar el elemento de la matriz\n",
    "    // que es calculado por el thread\n",
    "    float Pvalor = 0;\n",
    " \n",
    "    // Cada thread carga un renglon de M y una columna de N,\n",
    "    // para producir un elemento de P.\n",
    "    for (int k = 0; k < %(TAMANIO_MATRIZ)s; ++k) {\n",
    "        float Aelemento = a[ty * %(TAMANIO_MATRIZ)s + k];\n",
    "        float Belemento = b[k * %(TAMANIO_MATRIZ)s + tx];\n",
    "        Pvalor += Aelemento * Belemento;\n",
    "    }\n",
    "\n",
    "    // Escribe la matriz a la memoria del device;\n",
    "    // cada thread escribe un elemento.\n",
    "    c[ty * %(TAMANIO_MATRIZ)s + tx] = Pvalor;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# definimos en tamaño de la matriz\n",
    "TAMANIO_MATRIZ = 4\n",
    "\n",
    "# creamos dos matrices aleatorias\n",
    "a_cpu = np.random.randn(TAMANIO_MATRIZ, TAMANIO_MATRIZ).astype(np.float32)\n",
    "b_cpu = np.random.randn(TAMANIO_MATRIZ, TAMANIO_MATRIZ).astype(np.float32)\n",
    "\n",
    "# calculamos una referencia en el CPU para verificar el cálculo en el GPU\n",
    "c_cpu = np.dot(a_cpu, b_cpu)\n",
    "\n",
    "# transferimos del host (CPU) al device (GPU) \n",
    "a_gpu = gpuarray.to_gpu(a_cpu) \n",
    "b_gpu = gpuarray.to_gpu(b_cpu)\n",
    "\n",
    "# creamos un arreglo en gpu para guardar el resultado (C = A*B)\n",
    "c_gpu = gpuarray.empty((TAMANIO_MATRIZ, TAMANIO_MATRIZ), np.float32)\n",
    "\n",
    "# jalamos el código del kernel\n",
    "# especificando la constante TAMANIO_MATRIZ\n",
    "kernel_codigo = plantilla_codigo_kernel % {\n",
    "    'TAMANIO_MATRIZ': TAMANIO_MATRIZ \n",
    "    }\n",
    "\n",
    "# compilamos el código del kernel\n",
    "mod = compiler.SourceModule(kernel_codigo)\n",
    "\n",
    "# jalamos la función del kernel compilado\n",
    "matrixmul = mod.get_function(\"MatrizMulKernel\")\n",
    "\n",
    "# llamamos al kernel en la tarjeta\n",
    "matrixmul(\n",
    "    # entradas\n",
    "    a_gpu, b_gpu, \n",
    "    # salidas\n",
    "    c_gpu, \n",
    "    # bloque de TAMANIO_MATRIZ X TAMANIO_MATRIZ threads\n",
    "    block = (TAMANIO_MATRIZ, TAMANIO_MATRIZ, 1),\n",
    "    )\n",
    "\n",
    "# imprimimos los resultados\n",
    "print (\"-\" * 80)\n",
    "print (\"Matriz A (GPU):\")\n",
    "print (a_gpu.get())\n",
    "\n",
    "print (\"-\" * 80)\n",
    "print (\"Matriz B (GPU):\")\n",
    "print (b_gpu.get())\n",
    "\n",
    "print (\"-\" * 80)\n",
    "print (\"Matriz C (GPU):\")\n",
    "print (c_gpu.get())\n",
    "\n",
    "print (\"-\" * 80)\n",
    "print (\"Diferencia CPU-GPU:\")\n",
    "print (c_cpu - c_gpu.get())\n",
    "\n",
    "np.allclose(c_cpu, c_gpu.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Matrix A (GPU):\n",
      "[[-0.23232929 -2.14635515 -0.33738685 -0.28380695]\n",
      " [ 0.97448111  0.18537141 -0.05692802 -0.2662923 ]\n",
      " [ 1.2280252  -1.12965894 -0.07786353  0.96599358]\n",
      " [-0.02382421 -1.21478784  0.10618835  0.35622698]]\n",
      "--------------------------------------------------------------------------------\n",
      "Matrix B (GPU):\n",
      "[[-0.4823854  -0.45011848 -0.21072748 -1.90480971]\n",
      " [ 0.49980554  0.81493664  1.20101392  0.66717857]\n",
      " [ 0.73462611 -0.89565551  0.40500295  0.55907327]\n",
      " [-0.38914606 -0.77034527  0.24235159 -0.82585317]]\n",
      "--------------------------------------------------------------------------------\n",
      "Matrix C (GPU):\n",
      "[[-1.09809875 -1.12375593 -2.73426819 -0.94370019]\n",
      " [-0.31562001 -0.03144109 -0.07030869 -1.54443383]\n",
      " [-1.59010434 -2.14776707 -1.41293955 -3.93413877]\n",
      " [-0.65628082 -1.34877729 -1.32461798 -0.99992394]]\n",
      "--------------------------------------------------------------------------------\n",
      "Diferencia CPU-GPU:\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00  -3.72529030e-09   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [ -5.96046448e-08   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "Norma L2: 5.97209e-08\n",
      "CPU times: user 3.71 ms, sys: 215 µs, total: 3.92 ms\n",
      "Wall time: 3.82 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "\n",
    "# -- inicializamos el dispositivo\n",
    "import pycuda.autoinit\n",
    "\n",
    "plantilla_kernel_codigo = \"\"\"\n",
    "__global__ void MatrizMulKernel(float *A, float *B, float *C)\n",
    "{\n",
    "\n",
    "  const uint wA = %(TAMANIO_MATRIZ)s;\n",
    "  const uint wB = %(TAMANIO_MATRIZ)s;  \n",
    "  \n",
    "  // Indice de bloque\n",
    "  const uint bx = blockIdx.x;\n",
    "  const uint by = blockIdx.y;\n",
    "\n",
    "  // Indice de thread\n",
    "  const uint tx = threadIdx.x;\n",
    "  const uint ty = threadIdx.y;\n",
    "\n",
    "  // Indice de la primer submatriz de A procesada por el bloque\n",
    "  const uint aBegin = wA * %(TAMANIO_BLOQUE)s * by;\n",
    "  // Indice de la ultima submatriz de A procesada por el bloque\n",
    "  const uint aEnd = aBegin + wA - 1;\n",
    "  // Tamanio de paso para iterar sobre las submatrices de A\n",
    "  const uint aStep = %(TAMANIO_BLOQUE)s;\n",
    "\n",
    "  // Indice de la primer submatriz de B procesada por el bloque\n",
    "  const uint bBegin = %(TAMANIO_BLOQUE)s * bx;\n",
    "  // Tamanio de paso para iterar sobre las submatrices de B\n",
    "  const uint bStep = %(TAMANIO_BLOQUE)s * wB;\n",
    "\n",
    "  // El elemento de cada submatriz que el calculado por el thread\n",
    "  float Csub = 0;\n",
    "  // Ciclo sobre las submatrices de A y B\n",
    "  for (int a = aBegin, b = bBegin;\n",
    "       a <= aEnd;\n",
    "       a += aStep, b += bStep) \n",
    "    {\n",
    "      // Memoria compartida para la submatriz de A\n",
    "      __shared__ float As[%(TAMANIO_BLOQUE)s][%(TAMANIO_BLOQUE)s];\n",
    "      // Memoria compartida para la submatriz de B\n",
    "      __shared__ float Bs[%(TAMANIO_BLOQUE)s][%(TAMANIO_BLOQUE)s];\n",
    "\n",
    "      // Pasar las matrices de memoria global a compartida\n",
    "      As[ty][tx] = A[a + wA * ty + tx];\n",
    "      Bs[ty][tx] = B[b + wB * ty + tx];\n",
    "      // Sincronizar para asegurarnos de que se cargan completamente\n",
    "      // las matrices antes de comenzar a calcular\n",
    "      __syncthreads();\n",
    "\n",
    "      // Multiplicar las dos matrices\n",
    "      for (int k = 0; k < %(TAMANIO_BLOQUE)s; ++k)\n",
    "        Csub += As[ty][k] * Bs[k][tx];\n",
    "\n",
    "      // Sincronizar para asegurarse de que el calculo anterior\n",
    "      // ha finalizado\n",
    "      __syncthreads();\n",
    "    }\n",
    "\n",
    "  // Escribir la submatriz a la memoria global\n",
    "  const uint c = wB * %(TAMANIO_BLOQUE)s * by + %(TAMANIO_BLOQUE)s * bx;\n",
    "  C[c + wB * ty + tx] = Csub;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# definimos el tamaño de la matriz\n",
    "TAMANIO_MATRIZ = 4\n",
    "\n",
    "# definimos el tamaño de los bloques y las teselas\n",
    "TAMANIO_TILE = 2\n",
    "TAMANIO_BLOQUE = TAMANIO_TILE\n",
    "\n",
    "# creamos dos matrices cuadradas aleatorias\n",
    "a_cpu = np.random.randn(TAMANIO_MATRIZ, TAMANIO_MATRIZ).astype(np.float32)\n",
    "b_cpu = np.random.randn(TAMANIO_MATRIZ, TAMANIO_MATRIZ).astype(np.float32)\n",
    "\n",
    "# calculamos la referencia\n",
    "c_cpu = np.dot(a_cpu, b_cpu)\n",
    "\n",
    "# transferimos memoria del host al device\n",
    "a_gpu = gpuarray.to_gpu(a_cpu) \n",
    "b_gpu = gpuarray.to_gpu(b_cpu)\n",
    "\n",
    "# creamos un arreglo en blanco para guardar el resultado\n",
    "c_gpu = gpuarray.empty((TAMANIO_MATRIZ, TAMANIO_MATRIZ), np.float32)\n",
    "\n",
    "# jalamos el código del kernel especificando las constantes\n",
    "kernel_codigo = plantilla_kernel_codigo % { \n",
    "    'TAMANIO_MATRIZ': TAMANIO_MATRIZ,\n",
    "    'TAMANIO_BLOQUE': TAMANIO_BLOQUE,\n",
    "    }\n",
    "\n",
    "# compilamos el kernel\n",
    "mod = compiler.SourceModule(kernel_codigo)\n",
    "\n",
    "# jalamos la función del kernel\n",
    "matrixmul = mod.get_function(\"MatrizMulKernel\")\n",
    "\n",
    "# llamamos al kernel\n",
    "matrixmul(\n",
    "    # entradas\n",
    "    a_gpu, b_gpu, \n",
    "    # salidas\n",
    "    c_gpu, \n",
    "    # malla de varios bloques\n",
    "    grid = (TAMANIO_MATRIZ // TAMANIO_TILE, TAMANIO_MATRIZ // TAMANIO_TILE),\n",
    "    # bloque de varios threads\n",
    "    block = (TAMANIO_TILE, TAMANIO_TILE, 1), \n",
    "    )\n",
    "\n",
    "# Imprimimos los resultados\n",
    "print \"-\" * 80\n",
    "print \"Matrix A (GPU):\"\n",
    "print a_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Matrix B (GPU):\"\n",
    "print b_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Matrix C (GPU):\"\n",
    "print c_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Diferencia CPU-GPU:\"\n",
    "print c_cpu - c_gpu.get()\n",
    "print \"Norma L2:\", la.norm(c_cpu - c_gpu.get())\n",
    "np.allclose(c_cpu, c_gpu.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos apreciar, la multiplicación por teselas es más eficiente. \n",
    "\n",
    "Algo que es importante remarcar es el hecho de que uno estaría tentado a usar `double` en lugar de `float` en el kernel, sin embargo aún hay problemas para utilizar variables `double` en algunas tarjetas (tal es el caso de la tarjeta en que corremos esto).\n",
    "\n",
    "En las notas de Roberto Zamora podemos ver operaciones más complicadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materiales adicionales\n",
    "\n",
    "- [Notas de Roberto Zamora.](https://github.com/zamorays/miniCursoPycuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "- [Documentación de PyCUDA.](http://documen.tician.de/pycuda/)\n",
    "- [Wiki de PyCUDA](http://wiki.tiker.net/PyCuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
