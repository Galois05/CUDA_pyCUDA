{
 "metadata": {
  "name": "",
  "signature": "sha256:571585515506be5637c8c1f884bbf064d061b934d19ce88088119fafba66e5c6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introducci\u00f3n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El c\u00f3mputo en *unidades de procesamiento gr\u00e1fico (GPU por sus siglas en ingl\u00e9s)* pertenece a una de las tendencias m\u00e1s nuevas en el mundo de la ciencia computacional. Su atractivo realmente reside en el hecho de que actualmente las tarjetas gr\u00e1ficas cuentan con un gran poder computacional. Por poner un ejemplo, al momento de escribir esto (*mediados de 2014*) la tarjeta [*Nvidia GeForce GTX TITAN Z*](http://www.nvidia.com/gtx-700-graphics-cards/gtx-titan-z/)  que se muestra en la figura 1 (*est\u00e1 guapa \u00bfNo?*) tiene **5760 n\u00facleos** y **12 GB** de memoria, con un rendimiento computacional pico de **8.1Tflops** en aritm\u00e9tica de precisi\u00f3n simple. Para poner \u00e9ste resultado en contexto [*Kan Balam*](http://es.wikipedia.org/wiki/Kan_Balam) la supercomputadora instalada en 2007 en la *UNAM* contaba con **7113GFlops**, i.e. un poco menos del poder computacional de la *Nvidia GeForce GTX TITAN Z*; la actual supercomputadora de la *UNAM*,  [*Miztli*](http://www.super.unam.mx/index.php/content-layouts?start=1) cuenta con **21.3Tflops** esto es aproximadamente **2.63** veces el poder computacional de la *GeForce GTX TITAN Z*.\n",
      "\n",
      "\n",
      "\n",
      "<img src=\"Titan-z.jpg\">\n",
      "\n",
      "<h6 align=\"center\">Figura 1: Nvidia GeForce GTX TITAN Z </h6>\n",
      "\n",
      "Pero es necesario hacer ciertas aclaraciones, en primer lugar, estas tarjetas son demasiado caras, y esto significa que uno no es capaz de comprarlas con lo que le paga su pap\u00e1 al lavar el coche. La tarjeta que se muestra en la figura 1 tiene como precio de lanzamiento **$2,999** y no son pesos, sino d\u00f3lares. De esta manera, el c\u00f3mputo intensivo en GPUs se sale del alcance del ama de casa promedio; as\u00ed \u00e9sta clase de c\u00f3mputo se restringe y por lo tanto no se encuentra a la mano de todo aqu\u00e9l que desee experimentar con ella.\n",
      "\n",
      "Al leer esto surgir\u00e1 una pregunta natural, y es \u00bfpor qu\u00e9 no se hace todo el c\u00f3mputo en GPUs? Hemos visto que su poder computacional actualmente es enorme, qu\u00e9 hace a los gobiernos y las universidades invertir millones para comprar una supercomputadora si pueden gastar tres mil d\u00f3lares y comprar una tarjeta gr\u00e1fica poderosa, \u00bfla pol\u00edtica les ha da\u00f1ado? La respuesta no tiene que ver con esto, sino con los procesadores gr\u00e1ficos en s\u00ed. Para entender esto se necesita saber c\u00f3mo funcionan los GPUs y para que eso no sea aburrido habremos de adentrarnos en la historia de estos espectaculares bichos."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Procesadores gr\u00e1ficos y el surgimiento de CUDA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El c\u00f3mputo en paralelo ha estado entre nosotros de una forma u otra por d\u00e9cadas. En sus primeros pasos, estaba confinado a usuarios con acceso a grandes y costosas computadoras. Hoy en d\u00eda, las cosas son diferentes. Casi cualquier usuario que tenga en su poder una computadora de escritorio o una laptop tiene *unidades centrales de procesamiento* (*CPU*) con varios n\u00facleos, e.g. el procesador [Intel i7](http://www.intel.com/products/processor/corei7/specifications.htm) que cuenta con **4 n\u00facleos f\u00edsicos y 4 n\u00facleos l\u00f3gicos**. Casi todos los procesadores en tel\u00e9fonos celulares y tablets cuentan con varios n\u00facleos. La raz\u00f3n principal para la ubicuidad de n\u00facleos m\u00faltiples en los CPUs se debe al fracaso de los dise\u00f1adores de procesadores para aumentar el rendimiento en dise\u00f1os de solamente un n\u00facleo, esto se lograba t\u00edpicamente incrementando la velocidad del reloj, \u00e9sta \u00faltima nos da una buena idea del rendimiento del procesador, e.g una *computadora personal* (*PC*) con un CPU [Intel 80486](http://en.wikipedia.org/wiki/Intel_80486) corriendo a **50 MHz** ser\u00e1 aproximadamente dos veces m\u00e1s r\u00e1pida que una que utilice el mismo CPU y cuente con la misma memoria pero corra a **25 MHz**\n",
      ", no siempre se cumple esta relaci\u00f3n, pero en buena aproximaci\u00f3n podemos decir que la velocidad del reloj nos da una buena idea de la velocidad de la computadora que lo usa; si los procesadores son de la misma familia, entonces la relaci\u00f3n anterior es v\u00e1lida. Como resultado de la incapacidad para aumentar la velocidad del reloj, los fabricantes tuvieron que ingeni\u00e1rselas; como resultado de esto, desde aproximadamente 2005 los dise\u00f1os de CPUs han \"escalado\" a una mayor cantidad de n\u00facleos, en lugar de \"escalar\" a mayores velocidades de reloj. Hoy en d\u00edaest\u00e1n disponibles CPUs que cuentan desde un par de n\u00facleos hasta decenas de \u00e9stos, sin embargo esta candidad de n\u00facleos en paralelo palidece ante la cantidad de n\u00facleos disponibles en la *GeForce GTX TITAN Z*. Los GPUs fueron dise\u00f1ados como *arquitecturas* altamente paralelas, a mitades de la d\u00e9cada de 1990 (una *arquitectura* en este sentido se entiende las partes que conforman al GPU o al CPU y c\u00f3mo se relacionan dichas partes entre s\u00ed). Fueron dise\u00f1ados para trabajar as\u00ed principalmente debido a que el procesamiento de gr\u00e1ficos es una actividad inherentemente paralela. La afirmaci\u00f3n anterior m\u00e1s o menos obvia si se piensa con cuidado, dado que en cierto sentido, cada pixel de la pantalla debe funcionar independientemente de sus vecinos; para ejemplificar esto necesitaremos la ayuda del plomero de la figura 2.\n",
      "\n",
      "<img src=\"8_Bit_Mario.png\">\n",
      "\n",
      "<h6 align=\"center\">Figura 2: Mario en 8 bits </h6>\n",
      "\n",
      "Como podemos ver, por ejemplo la mano derecha de Mario consta de 7 cuadros, 6 de ellos forman un rect\u00e1ngulo y el \u00faltimo debe ser su pulgar y est\u00e1 colocado al lado del rect\u00e1ngulo, ahora bien, a un lado del pulgar se ve el oberol de mario, este es rojo y no se encuentra de alguna manera relacionado con el color de la mano (a menos que est\u00e9 desnudo), por lo tanto necesita libertad para mostrar cualquier color, de hecho, siguiendo este razonamiento cada uno de los pixeles de la pantalla por separado debe estar mostrando el color que el dibujante le asign\u00f3 para poder obtener la imagen deseada. Las tarjetas gr\u00e1ficas son la justicia computacional en persona, se encargan de que las \u00f3rdenes dadas por el dibujante (el sistema) sean ejecutadas en la pantalla, es por esto que los GPUs fueron dise\u00f1ados para funcionar en paralelo, dado que el hecho de formar im\u00e1genes en una pantalla exige que cada pixel se encuentre trabajando de manera independiente de sus colegas.\n",
      "\n",
      "El uso de GPUs para *c\u00f3mputo de prop\u00f3sitos generales* (*GPGPU*), fue en sus inicios una serie de retos duros de matar. Se ten\u00eda que programar a la *interfaz de programaci\u00f3n de aplicaciones gr\u00e1ficas* (*API*), que demostr\u00f3 ser muy restrictiva en la clase de algoritmos que pod\u00edan ser mapeados al GPU. Incluso cuando dicho mapeo era posible, la programaci\u00f3n requerida para hacer esto realidad era dif\u00edcil y para nada intuitiva para los cient\u00edficos e ingenieros totalmente desligados a la vocaci\u00f3n de los gr\u00e1ficos de computadora. Ve\u00e1moslo desde el punto de vista de estas pobres almas atormentadas.Las tarjetas gr\u00e1ficas en ese entonces ten\u00edan \u00f3rdenes muy precisas, colorea este pixel de amarillo, aqu\u00e9l de rojo, aqu\u00e9l de azul, aqu\u00e9l de (*inserte aqu\u00ed cualquiera de los dem\u00e1s colores que no me s\u00e9*), etc. Lo que quiere decir que si se deseaba programar en la tarjeta gr\u00e1fica se ten\u00eda que hacer un mapeo muy inteligente, por ejemplo declarar que un color fuera asignado a un n\u00famero y luego sumar colores. Uno cree que es f\u00e1cil, pero intuitivamente derivar un pixel morado y que d\u00e9 uno rojo simplemente es dif\u00edcil de entender, no s\u00f3lo eso, se hacen muchas operaciones con colores, y el resultado es otro color, tal vez esto ser\u00eda emocionante para *Salvador Dal\u00ed*, pero a decir verdad no era un espect\u00e1culo especialmente cautivador para los cient\u00edficos. Era demasiado trabajo y esfuerzo y somos muy flojos. Esto explica por qu\u00e9 el proceso de adoptar a los GPUs para c\u00f3mputo cient\u00edfico fue lento al principio.\n",
      "\n",
      "El horizonte se ilumin\u00f3 para el c\u00f3mputo en GPUs con la llegada de la arquitectura de **NVIDIA CUDA** en 2007. La arquitectura **CUDA** inclu\u00eda tanto componentes de *hardware* de los GPUs de NVIDIA y un ambiente de programaci\u00f3n de software que eliminaba las barreras que limitaron la adopci\u00f3n del GPGPU. Desde su primera aparici\u00f3n en 2007, CUDA tuvo una aceptaci\u00f3n tremenda, hasta el punto en que, en noviembre de 2010, tres de las supercomputadoras en el top cinco enlistadas en el top 500 usaban GPUs. En las lista de noviembre de 2012 la supercomputadora m\u00e1s r\u00e1pida contaba con el poder de los GPUs. Una de las razones para su r\u00e1pida adopci\u00f3n es que el modelo de programaci\u00f3n era simple. **CUDA C** la primera interfaz a la arquitectura CUDA, es b\u00e1sicamente **C** con unas pocas extensiones que permiten cargar porciones de un algoritmo para ser corridas en el GPU. Es entonces un acercamiento h\u00edbrido en donde tanto GPU como CPU son usados.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Aplicaciones de CUDA"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Im\u00e1gen m\u00e9dica"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Actualmente CUDA es usado en distintas \u00e1reas de la ciencia, en medicina se utiliza para detectar c\u00e1ncer de mama, originalmente este era detectado por mamograf\u00eda o rayos X, sin embargo frecuentemente estas pr\u00e1cticas llevaban a falsos positivos. La introducci\u00f3n de el primer GPU de NVIDIA con arquitectura CUDA junto con el lenguaje de programaci\u00f3n CUDA C, di\u00f3 una plataforma en la cual [TechniScan](http://techniscan.wordpress.com/) pudo convertir los sue\u00f1os de sus fundadores en realidad. Como su nombre lo indica,  su sistema de imagen por ultrasonido utiliza estas ondas para dar una imagen del pecho de la paciente. El sistema de la TechniScan conf\u00eda en el uso de dos tarjetas [*NVIDIA Tesla C1060*](http://www.nvidia.com/docs/IO/43395/NV_DS_Tesla_C1060_US_Jun08_FINAL_LowRes.pdf) para procesar los 35GB de datos que se generan en un escaneo de 15 minutos. Gracias al poder\u00edo de las tarjetas Tesla, 20 minutos despu\u00e9s el m\u00e9dico puede manipular una imagen muy detallada y en tres dimensiones del seno de la paciente."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Din\u00e1mica computacional de fluidos"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Por muchos a\u00f1os, el dice\u00f1o de rotores altamente eficientes y navajas permaneci\u00f3 como obra de artes oscuras o algo por el estilo. B\u00e1sicamente debido a que el movimiento del aire o de fluidos alrededor de estos dispositivos es impresionantemente complejo, y con impresionante quiero decir tan impresionante como un cerdo karateka (tal vez CUDA tambi\u00e9n pueda darnos una simulaci\u00f3n en eso). Esta complejidad causaba que los dispositivos no pudieran ser simulados de manera efectiva utilizando formulaciones sencillas, sin embargo las simulaciones mejor apegadas a la realidad resultaron ser demasiado costosas computacionalmente hablando (al hablar de costos aqu\u00ed se quiere decir, tiempo de simulaci\u00f3n, consumo de memoria RAM, consumo de disco duro, uso de cpu, etc\u00e9tera).\n",
      "\n",
      "S\u00f3lo las supercomputadoras m\u00e1s grandes del mundo pod\u00edan so\u00f1ar con ofrecer recursos computacionales a la altura de los sofisticados m\u00e9todos num\u00e9ricos requeridos para desarrollar y validar dise\u00f1os. Dado que muy pocos ten\u00edan acceso a dichas computadoras, la inovaci\u00f3n y el desarrollo de los dispositivos mencionados se mantuvo de cierta forma estancado.\n",
      "\n",
      "La universidad de Cambridge, en una gran tradici\u00f3n iniciada por [Charles Babbage](http://es.wikipedia.org/wiki/Charles_Babbage), es hogar de grupos de investigaci\u00f3n en el \u00e1rea de c\u00f3mputo en paralelo. [Graham Pullan](http://www.energy.cam.ac.uk/directory/gp10006@cam.ac.uk) y [Tobias Brandvick](http://www.many-core.group.cam.ac.uk/people/brandvik.shtml) del grupo \"many-core\" identificaron correctamente el potencial en la arquitectura CUDA para acelerar la din\u00e1mica computacional de fluidos a niveles impensables. Sus investigaciones iniciales indicaron que se pod\u00edan alcanzar niveles aceptables de rendimiento si se usaban *estaciones de trabajo* (*workstations*) personales con GPUs integrados. Posteriormente, el uso de peque\u00f1os clusters de GPUs f\u00e1cilmente superaron en rendimiento a sus costosas supercomputadoras y confirmaron las sospechas de que las capacidades de los GPUs se adecuaban bastante bien al tipo de problemas que trataban de resolver.\n",
      "\n",
      "Para los investigadores en Cambridge, las ganancias masivas en rendimiento provistas por CUDA C representaron m\u00e1s que una simple mejora a sus recursos de superc\u00f3mputo. La disponibilidad de cantidades grandes de GPUs de bajo costo (comparado con el de las supercomputadoras) le di\u00f3 a los investigadores el poder de hacer experimentaci\u00f3n r\u00e1pida. El hecho de recibir resultados en segundos estimul\u00f3 la llegada de avances importantes en esa \u00e1rea. Como resultado, el uso de clusters de GPUs ha transformado fundamentalmente la manera en que se hace la investigaci\u00f3n computacional en fluidos. El tener simulaciones casi interactivas ha desatado nuevas oportunidades para la inovaci\u00f3n y creatividad en un \u00e1rea de investigaci\u00f3n anteriormente varada."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "PyCUDA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ahora bien, el lector se encontrar\u00e1 dubitativo a estar alturas, y debe estar lleno de preguntas como: \u00bfEs tan maravilloso CUDA?, \u00bfqui\u00e9n es ese tal PyCUDA que anuncia el t\u00edtulo?, \u00bfes f\u00e1cil programar en CUDA?, \u00bfya conozco un lenguaje de programaci\u00f3n, no puedo usar ese para controlar el GPU? entre otras.\n",
      "Es mejor ir paso a paso, primero CUDA es maravilloso pero puede llegar a ser un dolor de cabeza, en eso no difiere de ning\u00fan lenguaje de programaci\u00f3n conocido, posteriormente veremos que no es recomendable utilizar el GPU para todo ya que puede aumentar los costes de los algoritmos.\n",
      "No se puede utilizar cualquier lenguaje de programaci\u00f3n (en principio) para comunicarse con el GPU, de la misma manera que no se puede encender la televisi\u00f3n con las llaves del auto, para mantener un dominio del GPU se necesita una interfaz que se comunique con el dispositivo, y como ya se vi\u00f3 CUDA es esa interfaz, aunque existen otras.\n",
      "Es f\u00e1cil programar en CUDA, la respuesta es *no*, sin embargo como en la mayor\u00eda de los casos lo que lo hace dif\u00edcil no es el lenguaje en s\u00ed, sino el hecho de enfrentarse con un nuevo paradigma, en resumen, lo que hace dif\u00edcil a CUDA es el hecho de que los algoritmos que hagamos deben estar dise\u00f1ados para ejecutarse en paralelo, y eso implica un nuevo enfoque cuando se habla de resoluci\u00f3n de problemas."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}